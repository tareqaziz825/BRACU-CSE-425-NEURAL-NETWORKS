# Comparative Evaluation of Clustering Algorithms on the Wine Dataset with Stability Analysis

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)](https://scikit-learn.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **CSE 425: Neural Networks Project**  
> Department of Computer Science and Engineering, BRAC University

## üìã Table of Contents
- [Overview](#overview)
- [Research Questions](#research-questions)
- [Methodology](#methodology)
- [Model Architecture](#model-architecture)
- [Experimental Setup](#experimental-setup)
- [Results](#results)
- [Key Findings](#key-findings)
- [Installation](#installation)
- [Usage](#usage)
- [Acknowledgements & Supervision](#acknowledgements--supervision)
- [References](#references)

## üî¨ Overview

This project introduces a **Stochastic Clustering Neural Network (SCNN)**, a novel unsupervised learning model inspired by Variational Autoencoders (VAEs), designed to perform uncertainty-aware clustering on tabular data. Unlike traditional deterministic clustering methods, the SCNN incorporates stochasticity directly into latent representations, enabling explicit quantification of cluster assignment uncertainty.

### Problem Statement

Traditional clustering algorithms like K-Means and Gaussian Mixture Models (GMMs) suffer from several limitations:
- **Sensitivity to initialization**: Often converge to local minima
- **Lack of uncertainty quantification**: Provide single cluster assignments without confidence measures
- **Limited stability**: Performance varies significantly across different random seeds
- **Deterministic nature**: Cannot capture ambiguity in overlapping clusters

### Our Contribution

We address these challenges by:
1. **Designing a VAE-inspired stochastic clustering network** that learns probabilistic latent representations
2. **Integrating uncertainty quantification** as an evaluation dimension for clustering tasks
3. **Conducting comprehensive stability analysis** across multiple random seeds
4. **Comparing against classical baselines** (K-Means, GMM, SOM) to highlight trade-offs between accuracy and stochastic interpretability

## üéØ Research Questions

This study investigates three fundamental questions:

1. **Can a stochastic neural network model improve clustering robustness compared to deterministic methods?**
   - Exploring whether VAE-inspired architectures provide more reliable cluster assignments

2. **How does incorporating non-determinism affect uncertainty quantification and stability?**
   - Measuring the impact of stochastic latent representations on clustering consistency

3. **What insights emerge from comparing SCNN with classical baselines?**
   - Evaluating trade-offs between K-Means, GMMs, SOMs, and the proposed SCNN

## üîç Methodology

### Dataset: Wine Dataset

- **Samples**: 178 wine instances
- **Features**: 13 chemical measurements (alcohol, malic acid, ash, alkalinity, magnesium, phenols, flavanoids, etc.)
- **Classes**: 3 wine cultivars
- **Preprocessing**: Z-score normalization (zero mean, unit variance)
- **Split**: 70% training, 30% testing

### Clustering Algorithms Evaluated

| Algorithm | Type | Key Characteristics |
|-----------|------|---------------------|
| **SCNN (Proposed)** | Stochastic Neural Network | VAE-inspired, uncertainty-aware, probabilistic latent space |
| **K-Means** | Centroid-based | Fast, deterministic, Euclidean distance minimization |
| **Gaussian Mixture Model (GMM)** | Probabilistic | Assumes Gaussian distributions, soft clustering |
| **Self-Organizing Map (SOM)** | Neural Network | Topology-preserving, competitive learning |

### Evaluation Metrics

1. **Silhouette Score**: Measures intra-cluster cohesion and inter-cluster separation
   - Range: [-1, 1]
   - Higher is better

2. **Adjusted Rand Index (ARI)**: Compares clustering with ground truth, adjusted for chance
   - Range: [-1, 1]
   - Perfect agreement: 1.0

3. **Normalized Mutual Information (NMI)**: Quantifies shared information between predicted and true labels
   - Range: [0, 1]
   - Perfect agreement: 1.0

4. **Stability**: Average pairwise ARI across multiple runs with different random seeds
   - Measures consistency and robustness

## üèóÔ∏è Model Architecture

### Stochastic Clustering Neural Network (SCNN)

The SCNN follows a Variational Autoencoder architecture with three main components:

#### 1. Encoder Network
```
Input (d dimensions) ‚Üí FC Layer (64 units, ReLU)
                     ‚Üì
                     Split
                    ‚Üô     ‚Üò
              Mean (Œº)    Log-Variance (log œÉ¬≤)
                    ‚Üò     ‚Üô
                  Latent Space (32 dimensions)
```

The encoder maps input features **x ‚àà ‚Ñù^d** to latent distribution parameters:
- **Mean vector**: Œº ‚àà ‚Ñù^l
- **Log-variance vector**: log œÉ¬≤ ‚àà ‚Ñù^l

#### 2. Reparameterization Trick

To enable backpropagation through stochastic sampling:
```
z = Œº + œÉ ‚äô Œµ,  where Œµ ~ N(0, I)
```

This formulation ensures differentiability for gradient-based optimization.

#### 3. Decoder Network
```
Latent Vector (z) ‚Üí FC Layer (64 units, ReLU) ‚Üí Output (d dimensions, Sigmoid)
```

The decoder reconstructs the original input from the latent vector.

### Loss Function

The SCNN is trained using a composite loss function:
```
L = E[||x - xÃÇ||¬≤‚ÇÇ] + Œ≤ ¬∑ KL(q(z|x) || p(z))
```

Where:
- **Reconstruction Loss**: Mean Squared Error (MSE) between input and reconstruction
- **KL Divergence**: Regularizes latent space to follow standard Gaussian N(0, I)
- **Œ≤**: Hyperparameter controlling reconstruction vs. regularization trade-off

### Hyperparameters

| Parameter | Value |
|-----------|-------|
| **Hidden Dimension** | 64 |
| **Latent Dimension** | 32 |
| **Learning Rate** | 0.001 |
| **Optimizer** | Adam |
| **Batch Size** | 32 |
| **Max Epochs** | 100 |
| **Early Stopping Patience** | 10 |
| **Œ≤ (KL weight)** | 1.0 |

## üß™ Experimental Setup

### Implementation Details

- **SCNN**: Implemented in PyTorch with custom VAE architecture
- **K-Means & GMM**: Scikit-learn implementations
- **SOM**: Custom NumPy implementation (10√ó10 grid, 1000 iterations)

### Baseline Configurations

1. **K-Means**
   - Number of clusters: 3
   - Initialization: k-means++
   - Distance metric: Euclidean

2. **Gaussian Mixture Model (GMM)**
   - Number of components: 3
   - Covariance type: Full
   - Initialization: k-means

3. **Self-Organizing Map (SOM)**
   - Grid size: 10√ó10
   - Training iterations: 1000
   - Post-processing: K-Means clustering on node positions

### Stability Analysis Protocol

For each algorithm:
1. Run clustering with 10 different random seeds
2. Compute pairwise ARI between all runs
3. Calculate average pairwise ARI as stability metric
4. Lower variance indicates higher stability

## üìä Results

### Quantitative Performance Comparison

| Metric | SCNN (Proposed) | K-Means | GMM | SOM |
|--------|-----------------|---------|-----|-----|
| **Silhouette Score** | 0.1520 | **0.2753** | 0.2694 | -0.0206 |
| **ARI** | 0.3593 | **0.8293** | 0.7844 | 0.0320 |
| **NMI** | 0.4186 | **0.8230** | 0.7732 | 0.4026 |
| **Stability** | 0.4885 | 0.5916 | **0.5923** | 974.8990 ‚ö†Ô∏è |

**Legend**: 
- ‚úÖ **Bold**: Best performance for that metric
- ‚ö†Ô∏è **Warning**: Extreme instability detected

### Performance Analysis

#### 1. K-Means (Best Overall Accuracy)
- ‚úÖ **Highest ARI (82.93%)** and NMI (82.30%)
- ‚úÖ Excellent alignment with ground truth labels
- ‚úÖ Fast computation and simple interpretation
- ‚ö†Ô∏è Deterministic‚Äîno uncertainty quantification
- ‚ö†Ô∏è Assumes spherical clusters

#### 2. Gaussian Mixture Model (Second Best)
- ‚úÖ **Strong performance**: 78.44% ARI, 77.32% NMI
- ‚úÖ **Highest stability** (0.5923) among deterministic methods
- ‚úÖ Soft clustering with probabilistic assignments
- ‚úÖ Handles elliptical cluster shapes
- ‚ö†Ô∏è Computationally more expensive than K-Means

#### 3. SCNN (Proposed - Uncertainty-Aware)
- ‚úÖ **Unique capability**: Explicit uncertainty quantification
- ‚úÖ Probabilistic latent representations
- ‚úÖ Stability variance of 0.4885 reveals cluster ambiguities
- ‚ö†Ô∏è Lower accuracy: 35.93% ARI, 41.86% NMI
- ‚ö†Ô∏è Requires careful hyperparameter tuning
- **Trade-off**: Sacrifices accuracy for interpretability and uncertainty awareness

#### 4. Self-Organizing Map (Failure Case)
- ‚ùå **Extremely unstable**: Stability > 900
- ‚ùå Near-random performance: 3.20% ARI
- ‚ùå Negative Silhouette Score (-0.0206)
- ‚ùå Highly sensitive to initialization and hyperparameters
- **Not recommended** for this dataset

### Uncertainty Analysis

The SCNN's key advantage lies in **uncertainty quantification**:
```
Average Uncertainty Variance: 0.4885
```

This metric represents the variability in cluster assignments across multiple stochastic latent samples, revealing:
- **Ambiguous data points** that lie near cluster boundaries
- **Confidence levels** for each cluster assignment
- **Overlapping regions** where classification is uncertain

**Practical Value**: In real-world applications (medical diagnosis, fraud detection, quality control), knowing *when the model is uncertain* is as important as the prediction itself.

### Visualization Insights

**Qualitative Analysis**:
- **SCNN**: Reasonably distinct clusters with some overlap in latent space
- **K-Means & GMM**: Sharp, well-separated cluster partitions in raw feature space
- **SOM**: Failed to produce meaningful separations, scattered assignments

## üîë Key Findings

### Main Contributions

1. **Novel Stochastic Clustering Architecture**
   - First VAE-inspired clustering network with explicit uncertainty quantification
   - Probabilistic latent representations enable confidence-aware cluster assignments

2. **Comprehensive Stability Analysis**
   - Introduced multi-seed stability metric for clustering robustness evaluation
   - Revealed extreme sensitivity of SOM to initialization

3. **Trade-off Identification**
   - **Deterministic methods** (K-Means, GMM): High accuracy, no uncertainty
   - **Stochastic SCNN**: Lower accuracy, rich uncertainty information
   - **Critical insight**: Choice depends on application requirements

### Theoretical Insights

1. **Stochastic Representations Alone Are Insufficient**
   - Incorporating randomness doesn't automatically improve clustering accuracy
   - Need specialized clustering objectives (e.g., Deep Embedded Clustering loss)

2. **Uncertainty Quantification Has Value**
   - Even with lower accuracy, SCNN provides actionable insights about ambiguous clusters
   - Particularly valuable for high-stakes domains requiring confidence estimates

3. **Initialization Matters**
   - SOM's extreme instability highlights critical importance of robust initialization
   - Stochastic modeling offers one path to mitigating initialization sensitivity

### Limitations

1. **SCNN underperforms classical methods in accuracy**
   - Requires integration of clustering-specific loss functions
   - May need larger datasets to fully leverage neural network capacity

2. **SOM highly unstable on Wine dataset**
   - Sensitive to grid size, learning rate, and neighborhood function
   - Better suited for visualization tasks than clustering

3. **Computational cost**
   - SCNN requires more training time than classical methods
   - Trade-off between interpretability and efficiency

## üöÄ Installation

### Prerequisites
```bash
Python 3.8 or higher
pip package manager
```

### Clone Repository
```bash
git clone https://github.com/yourusername/CSE425-Clustering-Analysis.git
cd CSE425-Clustering-Analysis
```

### Install Dependencies
```bash
pip install -r requirements.txt
```

**requirements.txt**:
```
torch>=2.0.0
numpy>=1.21.0
pandas>=1.3.0
scikit-learn>=1.0.0
matplotlib>=3.4.0
seaborn>=0.11.0
scipy>=1.7.0
```

## üìñ Usage

### Running the SCNN Model
```python
import torch
from models.scnn import StochasticClusteringNN
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load Wine dataset
from sklearn.datasets import load_wine
data = load_wine()
X, y = data.data, data.target

# Preprocess
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42
)

# Initialize SCNN
model = StochasticClusteringNN(
    input_dim=13,
    hidden_dim=64,
    latent_dim=32,
    beta=1.0
)

# Train model
model.train_model(
    X_train, 
    epochs=100, 
    batch_size=32, 
    learning_rate=0.001
)

# Get latent representations
latent_vectors = model.encode(X_test)

# Cluster in latent space
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(latent_vectors)

# Evaluate
from sklearn.metrics import adjusted_rand_score, silhouette_score

ari = adjusted_rand_score(y_test, cluster_labels)
silhouette = silhouette_score(latent_vectors, cluster_labels)

print(f"ARI: {ari:.4f}")
print(f"Silhouette Score: {silhouette:.4f}")
```

### Running Baseline Comparisons
```python
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(X_test)

# GMM
gmm = GaussianMixture(n_components=3, random_state=42)
gmm_labels = gmm.fit_predict(X_test)

# Evaluation
print("K-Means ARI:", adjusted_rand_score(y_test, kmeans_labels))
print("GMM ARI:", adjusted_rand_score(y_test, gmm_labels))
```

### Stability Analysis
```python
import numpy as np
from itertools import combinations

def compute_stability(X, y_true, model_class, n_runs=10):
    """Compute clustering stability across multiple runs."""
    all_labels = []
    
    for seed in range(n_runs):
        model = model_class(n_clusters=3, random_state=seed)
        labels = model.fit_predict(X)
        all_labels.append(labels)
    
    # Compute pairwise ARI
    ari_scores = []
    for labels1, labels2 in combinations(all_labels, 2):
        ari = adjusted_rand_score(labels1, labels2)
        ari_scores.append(ari)
    
    return np.mean(ari_scores), np.std(ari_scores)

# Example usage
stability_mean, stability_std = compute_stability(X_test, y_test, KMeans)
print(f"K-Means Stability: {stability_mean:.4f} ¬± {stability_std:.4f}")
```

### Visualization
```python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce to 2D for visualization
pca = PCA(n_components=2)
X_2d = pca.fit_transform(X_test)

# Plot clusters
plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_test, cmap='viridis', alpha=0.6)
plt.title('Ground Truth')
plt.xlabel('PC1')
plt.ylabel('PC2')

plt.subplot(1, 3, 2)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)
plt.title('K-Means Clustering')
plt.xlabel('PC1')

plt.subplot(1, 3, 3)
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=gmm_labels, cmap='viridis', alpha=0.6)
plt.title('GMM Clustering')
plt.xlabel('PC1')

plt.tight_layout()
plt.show()
```

## üôè Acknowledgements & Supervision

### Instructor
**Moin Mostakim**  
Senior Lecturer  
Department of Computer Science and Engineering  
BRAC University  
üìß Email: mostakim@bracu.ac.bd

## üìö References

1. **Kingma, D. P., & Welling, M.** (2013). "Auto-Encoding Variational Bayes." *arXiv:1312.6114*.  
   [Link](https://arxiv.org/abs/1312.6114)

2. **Higgins, I., et al.** (2016). "Beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework." *ICLR*.  
   [Link](https://openreview.net/forum?id=Sy2fzU9gl)

3. **Xie, J., Girshick, R., & Farhadi, A.** (2016). "Unsupervised Deep Embedding for Clustering Analysis." *ICML*.  
   [Link](http://proceedings.mlr.press/v48/xieb16.pdf)

## üîÆ Future Work

1. **Enhanced Clustering Objectives**
   - Integrate Deep Embedded Clustering (DEC) loss to improve latent space separation
   - Explore joint optimization of reconstruction and clustering objectives

2. **Scalability and Generalization**
   - Extend framework to larger, high-dimensional datasets (images, text)
   - Test on multimodal data with complex cluster structures

3. **Advanced Uncertainty Modeling**
   - Leverage Bayesian deep learning for richer uncertainty quantification
   - Implement Monte Carlo Dropout for ensemble-based confidence estimation

4. **Practical Applications**
   - **Anomaly Detection**: Use uncertainty to identify outliers
   - **Exploratory Data Analysis**: Interactive visualization of cluster ambiguities
   - **Decision Support Systems**: Confidence-aware recommendations in healthcare, finance

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---
**Keywords**: Unsupervised Learning, Variational Autoencoders, Clustering, K-Means, Gaussian Mixture Models, Self-Organizing Maps, Uncertainty Quantification, Stability Analysis, Wine Dataset, Neural Networks
